{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the libraries that we will use during the implementation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt #we will use it to draw the learning curve after training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets define the network architecture for xo. We know that we have 2 input neurons and the output is 1 neuron. For this problem we will use 1 hidden layer only with 3 hidden neurons. So, our network architecture is 2-3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_NEURONS = 2\n",
    "HIDDEN_NEURONS = 3\n",
    "OUTPUT_NEURONS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of epochs will be 100000. Anyway we will put an error stopping condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_OF_EPOCHS = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of variables in tensorflow.\n",
    "\n",
    "1- Constant: A variable that its value doesn't change in the program. It remains constant in the whole graph operations\n",
    "2- Variable: Its value changes during the learning process, but it must be intialized in the beginning of the program.\n",
    "3- Placeholder: An input from the user. It waits a value from the user to be intialized with it.\n",
    "\n",
    "The network inputs are the data (0, 1) and the label. They both are placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(tf.float32, [None, 2])  specifies the datatype and the dimension of the data.\n",
    "#Since we don't know the number of the training data, we make it None which means it accepts any from the user.\n",
    "#2 specifies that we have 2 input bits\n",
    "x = tf.placeholder(tf.float32, [None, 2]) \n",
    "y_target = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are weights and bias between the neural network layers. We will intialize the weights variable for input-hidden weights by this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2-3-1\n",
    "input_hidden_weights = tf.Variable([[-0.99, 1.05, .19], [-0.43, -0.44, -0.30]]) #Init. from the given network\n",
    "input_hidden_bias = tf.Variable(tf.ones([HIDDEN_NEURONS])) # The bias is one for each hidden neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets feedforward and use Sigmoid function as an activation function in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_neurons_values = tf.matmul(x, input_hidden_weights) + input_hidden_bias\n",
    "hidden_activation_result = tf.nn.sigmoid(hidden_neurons_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same between the hidden-output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_output_weights = tf.Variable([[0.18], [1.11], [-0.26]]) #Init from the given network\n",
    "hidden_output_bias = tf.Variable(tf.ones([1])) # The bias is one for the output neuroan\n",
    "hidden_output_value = tf.matmul(hidden_activation_result, hidden_output_weights) + hidden_output_bias\n",
    " \n",
    "y_estimated = tf.nn.sigmoid(hidden_output_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feedforward step, we need to calculate the error of the output. I have used Mean Squared Error and Cross Entropy for that. The loss function is passed to the gradient descent for the optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_mean((y_target * tf.log(y_estimated)) + ((1 - y_target) * tf.log(1 - y_estimated)))\n",
    "mean_squared_error = 0.5 * tf.reduce_sum((tf.square(y_estimated - y_target)))\n",
    " \n",
    "# Learning Rate is 0.1\n",
    "# Switch cross_entropy by mean_squared_error if you want\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin a tensorflow session. The session models the tensorflow code and intialize the graph and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our work with tensorflow is almost finished, the only thing remaining is to give the input to the graph and get some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_training_data = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "y_training_labels = [[0], [1], [1], [0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data, for example the error, we just need to run the session, give the input and finally mention the variable we want to obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = session.run(cross_entropy, feed_dict={x: x_training_data, y_target: y_training_labels}) #Change cross_entropy if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain multiple variables from the session. Simply overload the function with the names of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "epochs = []\n",
    " \n",
    "for i in range(0, NUM_OF_EPOCHS):\n",
    "    session.run(train, feed_dict={x: x_training_data, y_target: y_training_labels})\n",
    " \n",
    "    if i % 10 == 0:\n",
    "        print \"Iteration number: \", i, \"\\n\"\n",
    "        error = session.run(cross_entropy, feed_dict={x: x_training_data, y_target: y_training_labels})\n",
    "        print \"Cost: \", error, \"\\n\"\n",
    "        errors.append(error)\n",
    "        epochs.append(i)\n",
    " \n",
    "    if error < 0.01:\n",
    "        print \"Input to hidden Weights\",session.run(input_hidden_weights, feed_dict={x:x_training_data,y_target: y_training_labels})\n",
    "        print \"Input to hidden bias\", session.run(input_hidden_bias, feed_dict={x: x_training_data, y_target: y_training_labels})\n",
    "        print \"Hidden to output weight\",session.run(hidden_output_weights,feed_dict={x:x_training_data, y_target:y_training_labels})\n",
    "        print \"Hidden to output bias\", session.run(hidden_output_bias, feed_dict={x: x_training_data, y_target: y_training_labels})\n",
    " \n",
    "        plt.title(\"Learning Curve using cross entropy cost function\")\n",
    "        print \"Cost: \", error, \"\\n\"\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.plot(epochs, errors)\n",
    "        plt.show()\n",
    " \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, tensorflow is a good library for those who are geeks of Machine Learning and Neural Networks, but make sure that you know the functions you use and how they works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
